{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import transformers as T\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from ignite.metrics import Rouge\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import jieba\n",
    "import matplotlib.pyplot as plt\n",
    "from opencc import OpenCC\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-5\n",
    "epochs = 25\n",
    "train_batch_size = 1\n",
    "validation_batch_size = 1\n",
    "test_batch_size = 1\n",
    "random_seed = 42\n",
    "max_length = 300\n",
    "\n",
    "# path setting that souldn't be changed\n",
    "tokenizer_path = \"./saved_tokenizer\"\n",
    "dataset_path = \"./train.xlsx\"\n",
    "test_dataset_path = './test.xlsx'\n",
    "model_path = \"./saved_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\randy\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./saved_tokenizer\\\\tokenizer_config.json',\n",
       " './saved_tokenizer\\\\special_tokens_map.json',\n",
       " './saved_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = T.AutoModelForSeq2SeqLM.from_pretrained(\"IDEA-CCNL/Randeng-BART-139M-SUMMARY\", cache_dir=\"./cache/\").to(device)\n",
    "tokenizer = T.AutoTokenizer.from_pretrained(\"IDEA-CCNL/Randeng-BART-139M-SUMMARY\", cache_dir=\"./cache/\", model_max_length=512)\n",
    "tokenizer.save_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "We done text recognition in the main text of the judgment document to mark it with labels, in order to enhance the performance of the fine tuned LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pattern recognition for the main judgment text\n",
    "facts_pattern = re.compile(r'(.+?)，本院判決如下：', re.DOTALL)\n",
    "arguments_pattern = re.compile(r'理由(.+?)據上論結', re.DOTALL)\n",
    "decisions_pattern_1 = re.compile(r'據上論結，(.+?)判決', re.DOTALL)\n",
    "decisions_pattern_2 = re.compile(r'如下：主文(.+?)。', re.DOTALL)\n",
    "\n",
    "cc = OpenCC('tw2sp')\n",
    "stopwords = set([\"的\", \"在\", \"是\", \"了\", \"和\"])\n",
    "chinese_number_pattern = r'[零一二三四五六七八九十百千万亿]+'\n",
    "\n",
    "def chinese_to_arabic_number(chinese_num):\n",
    "    chinese_digits = {\n",
    "        '零': 0, '一': 1, '二': 2, '三': 3, '四': 4,\n",
    "        '五': 5, '六': 6, '七': 7, '八': 8, '九': 9, \n",
    "        # '○': 0\n",
    "    }\n",
    "\n",
    "    chinese_units = {\n",
    "        '十': 10, '百': 100, '千': 1000,\n",
    "        '萬': 10000, '億': 100000000\n",
    "    }\n",
    "    \n",
    "    def convert_section(section):\n",
    "        num = 0\n",
    "        unit = 1\n",
    "        for char in reversed(section):\n",
    "            if char in chinese_units:\n",
    "                unit = chinese_units[char]\n",
    "            else:\n",
    "                digit = chinese_digits[char]\n",
    "                num += digit * unit\n",
    "                unit *= 10\n",
    "        return num\n",
    "\n",
    "    if chinese_num == '零':\n",
    "        return 0\n",
    "\n",
    "    sections = chinese_num.split('億')\n",
    "    num = 0\n",
    "\n",
    "    if len(sections) == 2:\n",
    "        num += convert_section(sections[0]) * 100000000\n",
    "        sections = sections[1].split('萬')\n",
    "    else:\n",
    "        sections = sections[0].split('萬')\n",
    "\n",
    "    if len(sections) == 2:\n",
    "        num += convert_section(sections[0]) * 10000\n",
    "        num += convert_section(sections[1])\n",
    "    else:\n",
    "        num += convert_section(sections[0])\n",
    "\n",
    "    return num\n",
    "\n",
    "def transfer_special_token(text):\n",
    "    token_mapping = {\n",
    "        '㈠': '一',\n",
    "        '㈡': '二',\n",
    "        '㈢': '三',\n",
    "        '㈣': '四',\n",
    "        '㈤': '五',\n",
    "        '㈥': '六',\n",
    "        '㈦': '七',\n",
    "        '㈧': '八',\n",
    "        '㈨': '九',\n",
    "        '㈩': '十'\n",
    "    }\n",
    "    \n",
    "    for token, replacement in token_mapping.items():\n",
    "        text = text.replace(token, replacement)\n",
    "        \n",
    "    return text\n",
    "    \n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\u4e00-\\u9fff\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def trad_to_zh(text):\n",
    "    cc = OpenCC('tw2sp')\n",
    "    zh_text = cc.convert(text)\n",
    "    \n",
    "    return zh_text\n",
    "\n",
    "def zh_to_trad(text):\n",
    "    cc = OpenCC('s2twp')\n",
    "    trad_text = cc.convert(text)\n",
    "    \n",
    "    return trad_text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    filtered_texts = [word for word in text if word not in stopwords]\n",
    "    \n",
    "    return filtered_texts\n",
    "\n",
    "# The function that does the text preprocessing for the custom dataset\n",
    "def preprocess_text(text):\n",
    "    # text = clean_text(text)\n",
    "    num_matches = re.findall(chinese_number_pattern, text)\n",
    "    for match in num_matches:\n",
    "        arabic_number = str(chinese_to_arabic_number(match))\n",
    "        text = text.replace(match, arabic_number)\n",
    "    text = transfer_special_token(text)\n",
    "    text = trad_to_zh(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = ''.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor(sample):\n",
    "    # 將模型的輸入和ground truth打包成Tensor\n",
    "    model_inputs = tokenizer.batch_encode_plus([each[\"origin_context\"] for each in sample], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    model_outputs = tokenizer.batch_encode_plus([each[\"summary\"] for each in sample], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    return model_inputs[\"input_ids\"].to(device), model_outputs[\"input_ids\"].to(device)\n",
    "\n",
    "class Legal_Judgment_Dataset(Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        data_df = pd.read_excel(dataset_path)\n",
    "        self.data = []\n",
    "        for index, row in data_df.iterrows():\n",
    "            main_text = str(row['裁判原文'])\n",
    "            \n",
    "            facts = facts_pattern.search(main_text)\n",
    "            if (facts):\n",
    "                facts = '<facts> ' + str(preprocess_text(str(facts.group(1)))) + ' </facts>'\n",
    "            else:\n",
    "                facts = '<facts>  </facts>'\n",
    "                \n",
    "            arguments = arguments_pattern.search(main_text)\n",
    "            if (arguments):\n",
    "                arguments = '<arguments> ' + str(preprocess_text(str(arguments.group(1)))) + ' </arguments>'\n",
    "            else:\n",
    "                arguments = '<arguments>  </arguments>'\n",
    "                \n",
    "            decisions = '<decisions> '\n",
    "            decisions_1 = decisions_pattern_1.search(main_text)\n",
    "            if (decisions_1):\n",
    "                decisions_1 = preprocess_text(str(decisions_1.group(1)))\n",
    "                decisions += str(decisions_1)\n",
    "            decisions_2 = decisions_pattern_2.search(main_text)\n",
    "            if (decisions_2):\n",
    "                decisions_2 = preprocess_text(str(decisions_2.group(1)))\n",
    "                decisions += str(decisions_2)\n",
    "            decisions += ' </decisions>'\n",
    "            \n",
    "            origin_context = \"summary: \" + f\"{facts} {arguments} {decisions}\"\n",
    "            summary = trad_to_zh(row['摘要'])\n",
    "            self.data.append({\"origin_context\": origin_context, \"summary\": summary})\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Legal_Judgment_Dataset()\n",
    "train_dataset, untrain_dataset = train_test_split(dataset, test_size=0.2, random_state=random_seed)\n",
    "test_dataset, validation_dataset = train_test_split(untrain_dataset, test_size=0.5, random_state=random_seed)\n",
    "summary_train = DataLoader(train_dataset, collate_fn=get_tensor, batch_size=train_batch_size, shuffle=True)\n",
    "summary_validation = DataLoader(validation_dataset, collate_fn=get_tensor, batch_size=validation_batch_size, shuffle=False)\n",
    "\n",
    "rouge = Rouge(variants=[\"L\", 2], multiref=\"best\")\n",
    "print(f\"Dataset example: \\n{train_dataset[0]} \\n{train_dataset[1]} \\n{train_dataset[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset, ep):\n",
    "    pbar = tqdm(dataset)\n",
    "    pbar.set_description(f\"Evaluating\")\n",
    "    loss_list = []\n",
    "    for inputs, targets in pbar:\n",
    "        loss = model(inputs, labels=targets).loss\n",
    "        loss_list.append(loss.item())\n",
    "        pbar.set_postfix(loss = loss.item())\n",
    "        outputs = [each.replace(\"<unk>\", \"\").replace(\"<pad>\",\"\") for each in tokenizer.batch_decode(model.generate(inputs, max_length=max_length))]\n",
    "        targets = [each.replace(\"<unk>\", \"\").replace(\"<pad>\",\"\") for each in tokenizer.batch_decode(targets)]\n",
    "        if ep % 10 == 0:\n",
    "            print(\"model generate examples:\")\n",
    "            print(f\"output: {outputs}\")\n",
    "            print(f\"target: {targets}\")\n",
    "        for out, tar in zip(outputs, targets):\n",
    "            sentence = \" \".join(jieba.cut(out)).split()\n",
    "            ground_truth = \" \".join(jieba.cut(tar)).split()\n",
    "            for s in sentence:\n",
    "                rouge.update(([s], [ground_truth]))\n",
    "            \n",
    "    return rouge.compute(), np.mean(np.array(loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history = np.array([])\n",
    "validation_history = np.array([])\n",
    "rouge_L_P_history = np.array([])\n",
    "rouge_L_R_history = np.array([])\n",
    "rouge_L_F_history = np.array([])\n",
    "rouge_2_P_history = np.array([])\n",
    "rouge_2_R_history = np.array([])\n",
    "rouge_2_F_history = np.array([])\n",
    "best_val_score = {'Rouge-L-P': 0.0, \n",
    "                  'Rouge-L-R': 0.0, \n",
    "                  'Rouge-L-F': 0.0, \n",
    "                  'Rouge-2-P': 0.0, \n",
    "                  'Rouge-2-R': 0.0, \n",
    "                  'Rouge-2-F': 0.0\n",
    "                  }\n",
    "\n",
    "def dict_compare(val):\n",
    "    if val['Rouge-L-P'] > best_val_score['Rouge-L-P'] or \\\n",
    "       val['Rouge-L-R'] > best_val_score['Rouge-L-R'] or \\\n",
    "       val['Rouge-L-F'] > best_val_score['Rouge-L-F'] or \\\n",
    "       val['Rouge-2-P'] > best_val_score['Rouge-2-P'] or \\\n",
    "       val['Rouge-2-R'] > best_val_score['Rouge-2-R'] or \\\n",
    "       val['Rouge-2-F'] > best_val_score['Rouge-2-F']:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "for ep in range(epochs):\n",
    "    model.train()\n",
    "    pbar = tqdm(summary_train)\n",
    "    pbar.set_description(f\"Training epoch [{ep+1}/{epochs}]\")\n",
    "    loss_list = []\n",
    "    for inputs, targets in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_ids=inputs, labels=targets).loss\n",
    "        loss_list.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_postfix(loss = loss.item())\n",
    "    training_history = np.append(training_history, np.mean(np.array(loss_list)))\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    validation_score, validation_loss = evaluate(model, summary_validation, ep)\n",
    "    validation_history = np.append(validation_history, validation_loss)\n",
    "    rouge_L_P_history = np.append(rouge_L_P_history, validation_score['Rouge-L-P'])\n",
    "    rouge_L_R_history = np.append(rouge_L_R_history, validation_score['Rouge-L-R'])\n",
    "    rouge_L_F_history = np.append(rouge_L_F_history, validation_score['Rouge-L-F'])\n",
    "    rouge_2_P_history = np.append(rouge_2_P_history, validation_score['Rouge-2-P'])\n",
    "    rouge_2_R_history = np.append(rouge_2_R_history, validation_score['Rouge-2-R'])\n",
    "    rouge_2_F_history = np.append(rouge_2_F_history, validation_score['Rouge-2-F'])\n",
    "    \n",
    "    scheduler.step(validation_loss)\n",
    "    if dict_compare(validation_score):\n",
    "        best_val_score = validation_score\n",
    "        torch.save(model, 'judgment_summarization.mod')\n",
    "    \n",
    "    print(f\"Rouge-2 score on epoch {ep}:\", validation_score)\n",
    "torch.save(model, f'{model_path}/ep{ep}.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Training and validation loss history\")\n",
    "plt.plot(training_history, label=\"Training Loss\")\n",
    "plt.plot(validation_history, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Rouge Score history\")\n",
    "plt.plot(rouge_L_P_history, label=\"Rouge-L-P\")\n",
    "plt.plot(rouge_L_R_history, label=\"Rouge-L-R\")\n",
    "plt.plot(rouge_L_F_history, label=\"Rouge-L-F\")\n",
    "plt.plot(rouge_2_P_history, label=\"Rouge-2-P\")\n",
    "plt.plot(rouge_2_R_history, label=\"Rouge-2-R\")\n",
    "plt.plot(rouge_2_F_history, label=\"Rouge-2-F\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.savefig(\"training_history.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_dataset):\n",
    "    model.eval()\n",
    "    pbar = tqdm(test_dataset)\n",
    "    pbar.set_description(f\"Testing\")\n",
    "\n",
    "    for idx, testdata in tqdm(test_dataset.iterrows(), total=test_dataset.shape[0]):\n",
    "        main_text = str(testdata['裁判原文'])\n",
    "            \n",
    "        facts = facts_pattern.search(main_text)\n",
    "        if (facts):\n",
    "            facts =  '<facts> ' + str(preprocess_text(str(facts.group(1)))) + ' </facts>'\n",
    "        else:\n",
    "            facts = '<facts>  </facts>'\n",
    "            \n",
    "        arguments = arguments_pattern.search(main_text)\n",
    "        if (arguments):\n",
    "            arguments = '<arguments> ' + str(preprocess_text(str(arguments.group(1)))) + ' </arguments>'\n",
    "        else:\n",
    "            arguments = '<arguments>  </arguments>'\n",
    "            \n",
    "        decisions = '<decisions> '\n",
    "        decisions_1 = decisions_pattern_1.search(main_text)\n",
    "        if (decisions_1):\n",
    "            decisions_1 = preprocess_text(str(decisions_1.group(1)))\n",
    "            decisions += str(decisions_1)\n",
    "        decisions_2 = decisions_pattern_2.search(main_text)\n",
    "        if (decisions_2):\n",
    "            decisions_2 = preprocess_text(str(decisions_2.group(1)))\n",
    "            decisions += str(decisions_2)\n",
    "        decisions += ' </decisions>'\n",
    "        \n",
    "        ori_data = \"summary: \" + f\"{facts} {arguments} {decisions}\"\n",
    "        inputs = tokenizer(ori_data, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        outputs_context = tokenizer.decode(model.generate(**inputs, max_length=max_length)[0])\n",
    "        outputs = outputs_context.replace(\"<unk>\", \"\").replace(\"<pad>\",\"\")\n",
    "        \n",
    "        outputs = zh_to_trad(outputs)\n",
    "        outputs = ''.join(outputs)\n",
    "        \n",
    "        test_dataset.loc[idx, \"摘要\"] = outputs\n",
    "    \n",
    "    return test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_excel(test_dataset_path)\n",
    "test_dataset['摘要'] = None\n",
    "result_data = test(model, test_dataset)\n",
    "result_data.to_excel(\"result.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_excel(test_dataset_path)\n",
    "test_dataset['摘要'] = None\n",
    "result_data = test(torch.load('judgment_summarization.mod'), test_dataset)\n",
    "result_data.to_excel(\"result_best.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best Rouge-2 score: {'Rouge-L-P': 0.7020392355826369, 'Rouge-L-R': 0.7578127853881308, 'Rouge-L-F': 0.7578127853881308, 'Rouge-2-P': 0.18435750596534944, 'Rouge-2-R': 0.19227887705056654, 'Rouge-2-F': 0.19227887705056654}\n"
     ]
    }
   ],
   "source": [
    "print(f\"best Rouge-2 score:\", best_val_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
