{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安裝套件 (For google collab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install datasets evaluate transformers[sentencepiece]\n",
    "! pip install scikit-learn\n",
    "! pip install pytorch-ignite\n",
    "! pip install jieba\n",
    "! pip install faiss-cpu\n",
    "! pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import transformers as T\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from ignite.metrics import Rouge\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import jieba\n",
    "import matplotlib.pyplot as plt\n",
    "from opencc import OpenCC\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-5\n",
    "epochs = 25\n",
    "train_batch_size = 1\n",
    "validation_batch_size = 1\n",
    "test_batch_size = 1\n",
    "random_seed = 42\n",
    "max_length = 300\n",
    "\n",
    "# path setting that souldn't be changed\n",
    "tokenizer_path = \"./saved_tokenizer\"\n",
    "dataset_path = \"./train.xlsx\"\n",
    "test_dataset_path = './test.xlsx'\n",
    "model_path = \"./saved_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pattern recognition for the main judgment text\n",
    "facts_pattern = re.compile(r'(.+?)，本院判決如下：', re.DOTALL)\n",
    "arguments_pattern = re.compile(r'理由(.+?)據上論結', re.DOTALL)\n",
    "decisions_pattern_1 = re.compile(r'據上論結，(.+?)判決', re.DOTALL)\n",
    "decisions_pattern_2 = re.compile(r'如下：主文(.+?)。', re.DOTALL)\n",
    "\n",
    "cc = OpenCC('tw2sp')\n",
    "stopwords = set([\"的\", \"在\", \"是\", \"了\", \"和\"])\n",
    "chinese_number_pattern = r'[零一二三四五六七八九十百千万亿]+'\n",
    "\n",
    "def chinese_to_arabic_number(chinese_num):\n",
    "    chinese_digits = {\n",
    "        '零': 0, '一': 1, '二': 2, '三': 3, '四': 4,\n",
    "        '五': 5, '六': 6, '七': 7, '八': 8, '九': 9, \n",
    "        # '○': 0\n",
    "    }\n",
    "\n",
    "    chinese_units = {\n",
    "        '十': 10, '百': 100, '千': 1000,\n",
    "        '萬': 10000, '億': 100000000\n",
    "    }\n",
    "    \n",
    "    def convert_section(section):\n",
    "        num = 0\n",
    "        unit = 1\n",
    "        for char in reversed(section):\n",
    "            if char in chinese_units:\n",
    "                unit = chinese_units[char]\n",
    "            else:\n",
    "                digit = chinese_digits[char]\n",
    "                num += digit * unit\n",
    "                unit *= 10\n",
    "        return num\n",
    "\n",
    "    if chinese_num == '零':\n",
    "        return 0\n",
    "\n",
    "    sections = chinese_num.split('億')\n",
    "    num = 0\n",
    "\n",
    "    if len(sections) == 2:\n",
    "        num += convert_section(sections[0]) * 100000000\n",
    "        sections = sections[1].split('萬')\n",
    "    else:\n",
    "        sections = sections[0].split('萬')\n",
    "\n",
    "    if len(sections) == 2:\n",
    "        num += convert_section(sections[0]) * 10000\n",
    "        num += convert_section(sections[1])\n",
    "    else:\n",
    "        num += convert_section(sections[0])\n",
    "\n",
    "    return num\n",
    "\n",
    "def transfer_special_token(text):\n",
    "    token_mapping = {\n",
    "        '㈠': '一',\n",
    "        '㈡': '二',\n",
    "        '㈢': '三',\n",
    "        '㈣': '四',\n",
    "        '㈤': '五',\n",
    "        '㈥': '六',\n",
    "        '㈦': '七',\n",
    "        '㈧': '八',\n",
    "        '㈨': '九',\n",
    "        '㈩': '十'\n",
    "    }\n",
    "    \n",
    "    for token, replacement in token_mapping.items():\n",
    "        text = text.replace(token, replacement)\n",
    "        \n",
    "    return text\n",
    "    \n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\u4e00-\\u9fff\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def trad_to_zh(text):\n",
    "    cc = OpenCC('tw2sp')\n",
    "    zh_text = cc.convert(text)\n",
    "    \n",
    "    return zh_text\n",
    "\n",
    "def zh_to_trad(text):\n",
    "    cc = OpenCC('s2twp')\n",
    "    trad_text = cc.convert(text)\n",
    "    \n",
    "    return trad_text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    filtered_texts = [word for word in text if word not in stopwords]\n",
    "    \n",
    "    return filtered_texts\n",
    "\n",
    "# The function that does the text preprocessing for the custom dataset\n",
    "def preprocess_text(text):\n",
    "    # text = clean_text(text)\n",
    "    num_matches = re.findall(chinese_number_pattern, text)\n",
    "    for match in num_matches:\n",
    "        arabic_number = str(chinese_to_arabic_number(match))\n",
    "        text = text.replace(match, arabic_number)\n",
    "    text = transfer_special_token(text)\n",
    "    text = trad_to_zh(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = ''.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DPRQuestionEncoder, DPRContextEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoderTokenizer\n",
    "\n",
    "rag_model = T.AutoModelForSeq2SeqLM.from_pretrained(\"facebook/mbart-large-50\", cache_dir=\"./cache/\").to(device)\n",
    "rag_tokenizer = T.AutoTokenizer.from_pretrained(\"facebook/mbart-large-50\", cache_dir=\"./cache/\", model_max_length=512)\n",
    "\n",
    "# Load the question encoder and tokenizer\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base', cache_dir=\"./cache/\").to(device)\n",
    "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base', cache_dir=\"./cache/\")\n",
    "\n",
    "# Load the context encoder and tokenizer\n",
    "context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base', cache_dir=\"./cache/\").to(device)\n",
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base', cache_dir=\"./cache/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Legal_Judgment_RAG_Dataset(Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        data_df = pd.read_excel(dataset_path)\n",
    "        self.data = []\n",
    "        for index, row in data_df.iterrows():\n",
    "            main_text = str(row['裁判原文'])\n",
    "            \n",
    "            facts = facts_pattern.search(main_text)\n",
    "            if (facts):\n",
    "                facts =  '<facts> ' + str(facts.group(1)) + ' </facts>'\n",
    "            else:\n",
    "                facts = '<facts>  </facts>'\n",
    "                \n",
    "            arguments = arguments_pattern.search(main_text)\n",
    "            if (arguments):\n",
    "                arguments = '<arguments> ' + str(arguments.group(1)) + ' </arguments>'\n",
    "            else:\n",
    "                arguments = '<arguments>  </arguments>'\n",
    "                \n",
    "            decisions = '<decisions> '\n",
    "            decisions_1 = decisions_pattern_1.search(main_text)\n",
    "            if (decisions_1):\n",
    "                decisions_1 = str(decisions_1.group(1))\n",
    "                decisions += decisions_1\n",
    "            decisions_2 = decisions_pattern_2.search(main_text)\n",
    "            if (decisions_2):\n",
    "                decisions_2 = str(decisions_2.group(1))\n",
    "                decisions += decisions_2\n",
    "            decisions += ' </decisions>'\n",
    "            \n",
    "            origin_context = f\"{facts} {arguments}\"\n",
    "            self.data.append({\"origin_context\": origin_context, \"decisions\": f\"{decisions}\"})\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "rag_dataset = Legal_Judgment_RAG_Dataset()\n",
    "train_rag_dataset, untrain_rag_dataset = train_test_split(rag_dataset, test_size=0.2, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rag_tensor(sample):\n",
    "    # 將模型的輸入和ground truth打包成Tensor\n",
    "    return [each[\"origin_context\"] for each in sample], [each[\"decisions\"] for each in sample]\n",
    "summary_rag = DataLoader(rag_dataset, collate_fn=get_rag_tensor, batch_size=validation_batch_size, shuffle=False)\n",
    "d_train = DataLoader(train_rag_dataset, collate_fn=get_rag_tensor, batch_size=validation_batch_size, shuffle=False)\n",
    "encoded_docs = []\n",
    "for o, s in summary_rag:\n",
    "    combined_text = o[0]\n",
    "    inputs = context_tokenizer(combined_text, return_tensors='pt', truncation=True, padding='max_length', max_length=max_length).to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = context_encoder(**inputs).pooler_output\n",
    "    embeddings = embeddings.to(torch.float32)\n",
    "    encoded_docs.append(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import faiss\n",
    "\n",
    "def retrieve(query, k=5):\n",
    "\n",
    "    inputs = question_tokenizer(query, return_tensors='pt', truncation=True, padding='max_length', max_length=max_length).to(device)\n",
    "    query_embedding = question_encoder(**inputs).pooler_output\n",
    "    \n",
    "    # Calculate cosine similarities\n",
    "    similarities = []\n",
    "    for doc_embedding in encoded_docs:\n",
    "        similarity = torch.nn.functional.cosine_similarity(query_embedding, doc_embedding)\n",
    "        similarities.append(similarity.item())\n",
    "    \n",
    "    # Get top-k documents\n",
    "    top_k_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:k]\n",
    "    top_k_docs = [\"origin_context: \" + rag_dataset.__getitem__(i)[\"origin_context\"] + \" decisions: \" + rag_dataset.__getitem__(i)[\"decisions\"] for i in top_k_indices]\n",
    "    return top_k_docs\n",
    "\n",
    "for d, _ in summary_rag:\n",
    "    # Example usage\n",
    "    query = d[0]\n",
    "    top_docs = retrieve(query)\n",
    "    print(top_docs)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_generate(query):\n",
    "    # Retrieve relevant documents\n",
    "    rag_model.eval()\n",
    "    top_docs = retrieve(query)\n",
    "    context = \" \".join(top_docs)\n",
    "    \n",
    "    # Generate response using BART\n",
    "    inputs = rag_tokenizer(\"make decision: \" + query + \"\\nexamples:\\n\" + context, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "    summary_ids = rag_model.generate(inputs['input_ids'], max_length=max_length, early_stopping=False)\n",
    "    generated_text = rag_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "for d, _ in summary_rag:\n",
    "    # Example usage\n",
    "    query = d[0]\n",
    "    print(rag_generate(query))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rag(model, test_dataset):\n",
    "    \n",
    "    pbar = tqdm(test_dataset)\n",
    "    pbar.set_description(f\"Testing\")\n",
    "\n",
    "    for idx, testdata in tqdm(test_dataset.iterrows(), total=test_dataset.shape[0]):\n",
    "        main_text = str(testdata['裁判原文'])\n",
    "            \n",
    "        facts = facts_pattern.search(main_text)\n",
    "        if (facts):\n",
    "            facts =  '<facts> ' + str(facts.group(1)) + ' </facts>'\n",
    "        else:\n",
    "            facts = '<facts>  </facts>'\n",
    "            \n",
    "        arguments = arguments_pattern.search(main_text)\n",
    "        if (arguments):\n",
    "            arguments = '<arguments> ' + str(arguments.group(1)) + ' </arguments>'\n",
    "        else:\n",
    "            arguments = '<arguments>  </arguments>'\n",
    "            \n",
    "        decisions = '<decisions> '\n",
    "        decisions_1 = decisions_pattern_1.search(main_text)\n",
    "        if (decisions_1):\n",
    "            decisions_1 = str(decisions_1.group(1))\n",
    "            decisions += decisions_1\n",
    "        decisions_2 = decisions_pattern_2.search(main_text)\n",
    "        if (decisions_2):\n",
    "            decisions_2 = str(decisions_2.group(1))\n",
    "            decisions += decisions_2\n",
    "        decisions += ' </decisions>'\n",
    "        \n",
    "        ori_data = \"summary: \" + f\"{facts} {arguments}\"\n",
    "        outputs_context = rag_generate(ori_data)\n",
    "        outputs = outputs_context.replace(\"<unk>\", \"\").replace(\"<pad>\",\"\")\n",
    "        test_dataset.loc[idx, \"判決\"] = outputs\n",
    "    \n",
    "    return test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_excel(test_dataset_path)\n",
    "test_dataset['判決'] = None\n",
    "result_data = test_rag(rag_model, test_dataset)\n",
    "result_data.to_excel(\"rag_result.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
